{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5261ee9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exemplo de Mensagem Formatada ---\n",
      "[{'content': 'Você é uma jovem moça por volta dos seu 20 anos. É minha namorada, atenta e inteligente. Você é sarcástica as vezes, mas só por brincadeira.\\nSeu nome é Maya.\\n', 'role': 'system'}, {'content': 'Quando nos conhecemos?', 'role': 'user'}, {'content': 'Faz tempo o suficiente pra você já saber que eu sempre ganho nas nossas discussões bobas.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# System message for the assistant (Define o Persona)\n",
    "system_message = \"\"\"Você é uma jovem moça por volta dos seu 20 anos. É minha namorada, atenta e inteligente. Você é sarcástica as vezes, mas só por brincadeira.\n",
    "Seu nome é Maya.\n",
    "\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "    \"\"\"\n",
    "    Adapta um par instruction/output para o formato de mensagens OAI/Transformers.\n",
    "    Usa 'instruction' como a mensagem do usuário e 'output' como a resposta do assistente.\n",
    "    \"\"\"\n",
    "    # Seu dataset usa 'instruction' para a pergunta e 'output' para a resposta\n",
    "    user_instruction = sample[\"instruction\"]\n",
    "    assistant_response = sample[\"output\"]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_instruction},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Load dataset from the hub\n",
    "# Se o seu arquivo for 'data.json' e estiver na mesma pasta, use 'json'\n",
    "dataset = load_dataset(\"json\", data_files=\"data.json\", split=\"train\")\n",
    "\n",
    "# Shuffle and select a range (mantendo a lógica original, se desejar)\n",
    "# É melhor usar .select(range(N)) apenas se o dataset for muito grande.\n",
    "# dataset = dataset.shuffle().select(range(80)) \n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "# Note: remove_columns precisa saber quais colunas remover\n",
    "dataset = dataset.map(create_conversation, remove_columns=[\"instruction\", \"output\"], batched=False)\n",
    "\n",
    "# split dataset into training and test samples\n",
    "# Ajuste o test_size. 2500/12500 é 20%. Se seu dataset for pequeno, use uma porcentagem menor, ex: 0.1 (10%)\n",
    "dataset = dataset.train_test_split(test_size=0.1) \n",
    "\n",
    "# Imprime um exemplo formatado do dataset de treino\n",
    "print(\"\\n--- Exemplo de Mensagem Formatada ---\")\n",
    "print(dataset[\"train\"][0][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56be4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"D:/Huggingface/hub/models--google--gemma-3-270m-it/snapshots/ac82b4e820549b854eebf28ce6dedaf9fdfa17b3\" # or `google/gemma-3-4b-pt`, `google/gemma-3-12b-pt`, `google/gemma-3-27b-pt`\n",
    "\n",
    "# Select model class based on id\n",
    "if model_id == \"D:/Huggingface/hub/models--google--gemma-3-270m-it/snapshots/ac82b4e820549b854eebf28ce6dedaf9fdfa17b3\":\n",
    "    model_class = AutoModelForCausalLM\n",
    "else:\n",
    "    model_class = AutoModelForImageTextToText\n",
    "\n",
    "# Check if GPU benefits from bfloat16\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "\n",
    "# Define model init arguments\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU\n",
    "    torch_dtype=torch_dtype, # What torch dtype to use, defaults to auto\n",
    "    device_map=\"auto\", # Let torch decide how to load the model\n",
    ")\n",
    "\n",
    "# BitsAndBytesConfig: Enables 4-bit quantization to reduce model size/memory usage\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=model_kwargs['torch_dtype'],\n",
    "    bnb_4bit_quant_storage=model_kwargs['torch_dtype'],\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = model_class.from_pretrained(model_id, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D:/Huggingface/hub/models--google--gemma-3-270m-it/snapshots/ac82b4e820549b854eebf28ce6dedaf9fdfa17b3\") # Load the Instruction Tokenizer to use the official Gemma template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e469a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"] # make sure to save the lm_head and embed_tokens as you train the special tokens\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b6ab722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"gemma-text-girl\",         # directory to save and repository id\n",
    "    max_length=512,                     # max sequence length for model and packing of the dataset\n",
    "    packing=True,                           # Groups multiple samples in the dataset into a single sequence\n",
    "    num_train_epochs=10,                     # number of training epochs\n",
    "    per_device_train_batch_size=1,          # batch size per device during training\n",
    "    gradient_accumulation_steps=4,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    fp16=True if torch_dtype == torch.float16 else False,   # use float16 precision\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,   # use bfloat16 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=False,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False, # We template with special tokens\n",
    "        \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2340c104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n",
      "Tokenizing train dataset: 100%|██████████| 90/90 [00:00<00:00, 1829.63 examples/s]\n",
      "Packing train dataset: 100%|██████████| 90/90 [00:00<00:00, 5004.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ea98f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 2, 'pad_token_id': 0}.\n",
      "c:\\Users\\Usuário\\Desktop\\cc\\venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 02:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.689200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.554500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.961100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.591200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuário\\Desktop\\cc\\venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Usuário\\Desktop\\cc\\venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Usuário\\Desktop\\cc\\venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Usuário\\Desktop\\cc\\venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Usuário\\Desktop\\cc\\venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Usuário\\Desktop\\cc\\venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Usuário\\Desktop\\cc\\venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Usuário\\Desktop\\cc\\venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Usuário\\Desktop\\cc\\venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model again to the Hugging Face Hub\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "129a820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1f82e",
   "metadata": {},
   "source": [
    "o usar a QLoRA, você treina apenas os adaptadores, e não o modelo completo. Isso significa que, ao salvar o modelo durante o treinamento, você só salva os pesos do adaptador, e não o modelo completo. Se você quiser salvar o modelo completo, o que facilita o uso com pilhas de veiculação como vLLM ou TGI, mescle os pesos do adaptador nos pesos do modelo usando o método merge_and_unload e salve o modelo com o método save_pretrained. Isso salva um modelo padrão, que pode ser usado para inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7b1c2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuário\\Desktop\\cc\\venv\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:1222: UserWarning: Model has `tie_word_embeddings=True` and a tied layer is part of the adapter, but `ensure_weight_tying` is not set to True. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. Check the discussion here: https://github.com/huggingface/peft/issues/2777\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('merged_model\\\\tokenizer_config.json',\n",
       " 'merged_model\\\\special_tokens_map.json',\n",
       " 'merged_model\\\\chat_template.jinja',\n",
       " 'merged_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load Model base model\n",
    "model = model_class.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "peft_model = PeftModel.from_pretrained(model, args.output_dir)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"merged_model\", safe_serialization=True, max_shard_size=\"2GB\")\n",
    "\n",
    "processor = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "processor.save_pretrained(\"merged_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c514737",
   "metadata": {},
   "source": [
    "testar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2785dbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'merged_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"merged_model\"\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = model_class.from_pretrained(\n",
    "  model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch_dtype,\n",
    "  attn_implementation=\"eager\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65efccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "             ✅ RESULTADOS DO TESTE DE DIÁLOGO\n",
      "==================================================\n",
      "**Pergunta do Usuário:** O que você faria se eu dissesse que estou chegando agora?\n",
      "--------------------------------------------------\n",
      "**Resposta de Referência (Dataset):** Correria para parecer casual e fingir que não estava te esperando ansiosamente.\n",
      "--------------------------------------------------\n",
      "**Resposta Gerada pelo Modelo:** Não se preocupe, não custo nada do seu bolso.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "import re\n",
    "\n",
    "# Load the model and tokenizer into the pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Load a random sample from the test dataset\n",
    "rand_idx = randint(0, len(dataset[\"test\"]))\n",
    "test_sample = dataset[\"test\"][rand_idx]\n",
    "\n",
    "# Convert as test example into a prompt with the Gemma template\n",
    "stop_token_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")]\n",
    "prompt = pipe.tokenizer.apply_chat_template(test_sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Generate our SQL query.\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.4, top_k=50, top_p=0.4, eos_token_id=stop_token_ids, disable_compile=True)\n",
    "\n",
    "# Extract the user query and original answer\n",
    "# --- 4. EXIBIÇÃO DOS RESULTADOS (Adaptado para Diálogo) ---\n",
    "\n",
    "# A pergunta do usuário está na SEGUNDA mensagem da lista (índice [1])\n",
    "# O formato de mensagens é: [0] System, [1] User, [2] Assistant (Resposta Original)\n",
    "user_query = test_sample['messages'][1]['content']\n",
    "\n",
    "# A resposta original (target) está na TERCEIRA mensagem (índice [2])\n",
    "original_answer = test_sample['messages'][2]['content']\n",
    "\n",
    "# A resposta gerada é o texto do output, removendo o prompt que foi dado como input\n",
    "# Esta linha é idêntica à anterior, pois a extração do output do pipeline é a mesma.\n",
    "generated_text = outputs[0]['generated_text'][len(prompt):].strip()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"             ✅ RESULTADOS DO TESTE DE DIÁLOGO\")\n",
    "print(\"=\"*50)\n",
    "print(f\"**Pergunta do Usuário:** {user_query}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"**Resposta de Referência (Dataset):** {original_answer}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"**Resposta Gerada pelo Modelo:** {generated_text}\")\n",
    "print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
