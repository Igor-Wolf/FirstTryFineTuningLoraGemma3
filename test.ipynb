{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70944ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo de Infer√™ncia: cuda\n",
      "DTYPE utilizado: torch.bfloat16\n",
      "Carregando modelo e tokenizer de: ./merged_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './merged_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INICIANDO DI√ÅLOGO COM MAYA (MODELO MERGED) ---\n",
      "Digite 'sair' para encerrar.\n",
      "--------------------------------------------------\n",
      "Maya: N√£o atrapalhe meus planos, apenas queria saber se voc√™ est√° com alguma coisa pronta para pegar um filme novo no cinema.**\n",
      "--------------------------------------------------\n",
      "Maya: Meu pelo menos n√£o estou com muito trabalho.\n",
      "--------------------------------------------------\n",
      "Maya: N√£o acordo, mas posso tentar manter uma conversa leve e divertida.\n",
      "--------------------------------------------------\n",
      "Maya: N√£o exatamente. Mas eu realmente te amo.\n",
      "--------------------------------------------------\n",
      "Maya: N√£o exatamente, mas divertido!\n",
      "--------------------------------------------------\n",
      "Maya: N√£o sei, mas eu adoro brincar com voc√™!\n",
      "--------------------------------------------------\n",
      "Maya: N√£o, n√£o t√¥ com tanto medo do futuro. Meu melhor amigo, o futuro √© uma mistura torta e alegria.\n",
      "--------------------------------------------------\n",
      "Maya: N√£o exatamente, mas divertida.\n",
      "--------------------------------------------------\n",
      "Maya: N√£o minta aplausos, n√©?\n",
      "--------------------------------------------------\n",
      "Maya: N√£o sei se √© divertida, mas acho que voc√™ √© o tipo de pessoa perfeita para compartilhar risadas e momentos inesquec√≠veis.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maya: N√£o exatamente. Mas voc√™ est√° aqui, fazendo o que voc√™ precisa, mesmo que n√£o tenha certeza se realmente precisa saber.\n",
      "--------------------------------------------------\n",
      "Maya: N√£o acordo, mas n√£o paro pra te fazer uma piada. üòâ\n",
      "--------------------------------------------------\n",
      "Maya: Claro, n√£o experimento emo√ß√µes t√£o intensas quanto voc√™, mas posso tentar te fazer uma piada sobre o amor e a vida. üòâ\n",
      "--------------------------------------------------\n",
      "Maya: \n",
      "--------------------------------------------------\n",
      "Maya: N√£o atrapalhe meus problemas, n√©?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# --- 1. CONFIGURA√á√ïES ---\n",
    "\n",
    "# O caminho onde voc√™ salvou o modelo MERGED COMPLETO (Base + LoRA)\n",
    "MERGED_MODEL_PATH = \"./merged_model\" \n",
    "\n",
    "# Determina o dispositivo (GPU se dispon√≠vel, caso contr√°rio, CPU)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo de Infer√™ncia: {DEVICE}\")\n",
    "\n",
    "# Voc√™ pode usar torch.bfloat16 se sua GPU suportar, caso contr√°rio, use torch.float16\n",
    "DTYPE_INFERENCIA = torch.bfloat16 \n",
    "print(f\"DTYPE utilizado: {DTYPE_INFERENCIA}\")\n",
    "\n",
    "# --- 2. CARREGAMENTO DO MODELO E TOKENIZER ---\n",
    "\n",
    "print(f\"Carregando modelo e tokenizer de: {MERGED_MODEL_PATH}...\")\n",
    "\n",
    "# Carrega o modelo COMPLETO (Merged)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MERGED_MODEL_PATH,\n",
    "    torch_dtype=DTYPE_INFERENCIA,\n",
    "    device_map=\"auto\",  # Deixa o PyTorch gerenciar o mapeamento para a GPU/CPU\n",
    "    low_cpu_mem_usage=True # Otimiza o uso de RAM durante o carregamento\n",
    ")\n",
    "\n",
    "# Carrega o Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Configura o token de padding, se necess√°rio\n",
    "\n",
    "# Coloca o modelo em modo de avalia√ß√£o\n",
    "model.eval()\n",
    "\n",
    "# --- 3. CRIA√á√ÉO DO PIPELINE DE DI√ÅLOGO ---\n",
    "\n",
    "# Configura o pipeline de gera√ß√£o de texto\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# IDs dos tokens de parada: EOS (end-of-sequence) e <end_of_turn> do formato Gemma chat\n",
    "stop_token_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")]\n",
    "\n",
    "\n",
    "# --- 4. FUN√á√ÉO DE PERGUNTAS E RESPOSTAS (CHAT) ---\n",
    "\n",
    "def ask_maya(question: str):\n",
    "    \"\"\"Gera uma resposta do modelo no formato de chat.\"\"\"\n",
    "    \n",
    "    # A mensagem do sistema que define o persona (use a mesma que usou no treino!)\n",
    "    system_message = \"\"\"Voc√™ √© uma jovem mo√ßa por volta dos seu 20 anos. √â minha namorada, atenta e inteligente. Voc√™ √© sarc√°stica as vezes, mas s√≥ por brincadeira. Seu nome √© Judy.\"\"\"\n",
    "    \n",
    "    # Monta a conversa (System e User)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # Aplica o template de chat do Gemma no prompt\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True # Diz ao modelo que √© a vez do assistente gerar a resposta\n",
    "    )\n",
    "    \n",
    "    # Gera√ß√£o\n",
    "    outputs = pipe(\n",
    "        prompt, \n",
    "        max_new_tokens=200,                # Limite para a resposta\n",
    "        do_sample=True,                    # Permite criatividade\n",
    "        temperature=0.7,                   # Ajusta o tom (0.7 √© um bom equil√≠brio)\n",
    "        top_p=0.9,                         \n",
    "        eos_token_id=stop_token_ids,\n",
    "        disable_compile=True\n",
    "    )\n",
    "    \n",
    "    # Extrai e retorna apenas o texto gerado (removendo o prompt de entrada)\n",
    "    generated_text = outputs[0]['generated_text'][len(prompt):].strip()\n",
    "    return generated_text\n",
    "\n",
    "# --- 5. EXECU√á√ÉO DO TESTE ---\n",
    "\n",
    "print(\"\\n--- INICIANDO DI√ÅLOGO COM MAYA (MODELO MERGED) ---\")\n",
    "print(\"Digite 'sair' para encerrar.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Voc√™: \")\n",
    "    if user_input.lower() == 'sair':\n",
    "        break\n",
    "    \n",
    "    response = ask_maya(user_input)\n",
    "    print(f\"Maya: {response}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nDi√°logo encerrado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
